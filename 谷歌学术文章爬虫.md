## 脚本1： 根据文章名搜索所有的文章 存入信息
注意：

1. 建议使用谷歌镜像
```python
from bs4 import BeautifulSoup               #网页解析，获取数据
import re                                   #正则表达式
import urllib.request,urllib.error          #指定URL，获取数据
import xlwt                                 #进行Excel操作
from urllib.error import URLError
import xlrd
import xlutils.copy                        #excel表的写入
from time import  sleep
import random

"""
作用：请求网页 得到html
参数：网址
"""
def askURL(url):
    user_agents = [
        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
        'Opera/9.25 (Windows NT 5.1; U; en)',
        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
        'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',
        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',
        'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',
        "Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7",
        "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 ",
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0'
    ]
    html = ""
    headers = {'user-agent':user_agents[random.randint(0,8)] }
    req = urllib.request.Request(url=url, headers=headers)
    for i in range(5):
        try:
            res = urllib.request.urlopen(req, timeout=7)
            html=res.read().decode('utf-8')
            break
        except URLError as e:
            if hasattr(e,"code"):
                print(e.code)
            if hasattr(e,"reason"):
                print(e.reason)
    return html
  
"""
  这是一些正则表达式的设置 可以和re.findall结合使用去查找和正则表达的相对应的结果
  可以根据自己要求去修改
"""
downLoadFileExist_reg=re.compile(r'<div class="gs_ggs gs_fl">')
downLoadFileLink_reg=re.compile(r'href="(.*?)">')
content_reg=re.compile(r'<div class="gs_ri">(.*?)<div class="gs_rs">')
fileLink_reg=re.compile(r'href="(.*?)" id="')
fileName_reg=re.compile(r'<a data-clk="hl=zh-CN&amp;sa=T&amp;ct=res&amp;.*?">(.*?)</a>')
periodicalTime_reg=re.compile(r'<div class="gs_a">.*?- (.*?)</div>')
quteNum_reg=re.compile(r'<div class="gs_fl">.*?hl=zh-CN">被引用次数：(.*?)</a>')


"""
作用：根据url去获取html并分析 调用了askURL方法。
参数：start_url和end_url构建url path指定存储地址  number:需要爬取的页数
注意63行的start_num指定起始爬的论文的序号（即上次爬虫中断的下一号 方便爬虫中断续爬）
注意64行的num 指定当前爬取的是第几个论文 方便根据该序号续写excel
"""
def getData(start_url,end_url,path，number):
    fileName_reg = re.compile(r'<a data-clk="hl=zh-CN&amp;sa=T&amp;ct=res&amp;.*?">(.*?)</a>')
    datalist=[]
    start_num=0
    num = 0
    for i in range(0,number):
        url=start_url+str(start_num)+end_url
        start_num = start_num + 20
        sleep(5)
        html=askURL(url)
        #逐一解析
        soup=BeautifulSoup(html,"html.parser")
        for item in soup.find_all('div',class_="gs_r gs_or gs_scl"):
            data=[]
            content_str = []
            item_str=str(item)
            #re通过正则表达式查找指定的字符串
            if re.findall(content_reg,item_str)!=[]:
                content_str=re.findall(content_reg,item_str)[0]
            if re.findall(fileName_reg,content_str) is None:
                print("none")
            if re.findall(fileName_reg,content_str)==[] or re.findall(fileName_reg,content_str)==None:
                fileName_reg = re.compile(r'<a data-clk="hl=zh-CN&amp;sa=T&amp;oi=ggp&amp;ct=res&amp.*?">(.*?)</a>')
            if re.findall(fileName_reg,content_str)!=[] or re.findall(fileName_reg,content_str)!=None:
                fileName = re.findall(fileName_reg, content_str)[0]
            print(re.findall(fileName_reg, content_str))
            fileName_reg = re.compile(r'<a data-clk="hl=zh-CN&amp;sa=T&amp;ct=res&amp;.*?">(.*?)</a>')
            fileName=re.sub('<b>','',fileName)
            fileName = re.sub('</b>', '', fileName)
            periodicalTime=re.findall(periodicalTime_reg,content_str)[0]
            periodicalTime = re.sub('<b>', '', periodicalTime)
            periodicalTime = re.sub('</b>', '', periodicalTime)
            periodicalTime = re.sub('\xa0', '', periodicalTime)
            m = 0
            time = ""
            flag = 0
            for char in periodicalTime:
                if flag == 1 and m < 5:
                        time = time + char
                        m = m + 1
                if char == ',':
                    flag = 1
            periodicalTime = time
            if re.findall(downLoadFileExist_reg,item_str)==[]:
                downLoadFilelink=''
            else:
                downLoadFilelink=re.findall(downLoadFileLink_reg,item_str)[0]          
            data.append(fileName)
            data.append(periodicalTime)
            data.append(downLoadFilelink)
            saveOneData(path,data,num)
            datalist.append(data)
            num = num + 1
    return datalist
  
#分条存储数据 
def saveOneData(savepath,data,num):
    print("save......")
    if num == 0:
        print("当前条数：", num)
        book=xlwt.Workbook(encoding='utf-8',style_compression=0)
        sheet=book.add_sheet('self-supervised',cell_overwrite_ok=False)
        col=('fileName','periodicalTime','downLoadFilelink')
        for i in range(0,len(data)):
          sheet.write(0,i,col[i])
        for j in range(0, len(data)):
            sheet.write(num + 1, j, data[j])
        book.save(savepath)
    else:
        rd = xlrd.open_workbook(savepath, formatting_info = True)
        wt = xlutils.copy.copy(rd)
        sheets = wt.get_sheet(0)
        print("当前条数：",num)
        for j in range(0, len(data)):
            sheets.write(num + 1, j, data[j])
        wt.save(savepath)
"""
start_url 推荐使用国内镜像
end_url 根据自身要求改变参数
savepath 存储结果文件路径
"""     
def main():
    start_url="https://scholar.lanfanshu.cn/scholar?start="
    end_url="&q=test+case+prioritization&hl=zh-CN&as_sdt=0,5&as_vis=1&num=20"
    savepath=".\\1.xls"
    getData(start_url,end_url,savepath) 
    
if __name__=="__main__":
    main()
  
  
```


## 脚本2 ：根据论文名，模拟浏览器点击去获取bib 
由于bib无法直接从网页爬取，所以需要使用selenium工具去模拟打开浏览器点击爬取
注意：

1. 本脚本是第一个脚本结果的后续。如果没有运行第一个脚本在数据读入处修改即可。
1. 推荐使用镜像。
1. 本脚本对网络要求很高。所以爬丢的概率很高，可以通过延长sleep时间稍微提高一点成功率。



```python
from urllib import parse
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
import time
import xlrd
import xlwt
import xlutils.copy
import random

"""
得到url
"""
def paperUrl(name):
    q = name
    params = {
        'q':q
    }
    params = parse.urlencode(params)
    url = ""+params
    return url

"""
获取Bib 
如果成功就返回bib
失败就返回该条目在表里的序号num
"""
def getBib(url,num):
    user_agents = [
        'User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
        'User-Agent: Opera/9.25 (Windows NT 5.1; U; en)',
        'User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
        'User-Agent: Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',
        'User-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',
        'User-Agent: Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',
        "User-Agent: Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7",
        "User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 ",
        'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0'
    ]
    hr = ""
    bib = ""
    options = Options()
    options.add_argument('-headless')
    options.add_argument(user_agents[random.randint(0,8)])
    driver = webdriver.Firefox(options=options)
    driver.get(url)   
    for i in range(3):
        try:
            driver.find_element_by_class_name('gs_or_cit.gs_nph').click()
            time.sleep(3)
            s = driver.find_element_by_class_name('gs_citi')
            time.sleep(3)
            if s == '':
                s = driver.find_element_by_class_name('gs_citd')
            if s.text == 'BibTeX':
                hr = s.get_attribute('href')
            driver.get(hr)
            bib = driver.find_element_by_xpath("//*").text
            for i in range(5):
                try:
                    driver.quit()
                    break
                except:
                    time.sleep(0.1)
            break
        except:
            time.sleep(0.2)
    if bib=="":
        return num
    else:
        return bib

"""
将数据单条存入
"""
def saveOneData(savepath,data,num):
    print("save......")
    if num == 0:
        print("当前条数：", num)
        book=xlwt.Workbook(encoding='utf-8',style_compression=0)
        sheet=book.add_sheet('self-supervised',cell_overwrite_ok=False)
        col=('blb','name','year','title')
        for i in range(0,4):
          sheet.write(0,i,col[i])
        for j in range(0, len(data)):
            sheet.write(num + 1, j, data[j])
        book.save(savepath)
    else:
        rd = xlrd.open_workbook(savepath, formatting_info = True)
        wt = xlutils.copy.copy(rd)
        sheets = wt.get_sheet(0)
        print("当前条数：",num)
        for j in range(0, len(data)):
            sheets.write(num + 1, j, data[j])
        wt.save(savepath)

if __name__ == '__main__':
    savepath = ".\\savepath.xls"
    excel = xlrd.open_workbook(savepath) 
    sheet = excel.sheet_by_index(0)  
    rows: list = sheet.row_values(0) 
    index = rows.index('fileName') 
    listindes = sheet.col_values(index)  
    # 遍历该列所有的内容
    List = []
    errors = []
    #指定序数
    num = 0
    for i in range(1, len(listindes)):
        List.append(listindes[i])
    for q in List:
        url = paperUrl(q)
        bib = getBib(url,num)
        if type(bib)==int:
            print(url)
            print("error",bib)
            errors.append(bib)
        else:
            print(url)
            print(bib)
            bib = bib.splitlines(True)
            dic = {}
            for sub in bib[1:len(bib) - 1]:
                info = sub.split('=')
                dic[info[0]] = info[1]
            flag = 0
            data = ""
            def toString(string):
                newString = ""
                for s in string:
                    newString = newString + s
                return newString
            #这里简单分析bib的信息 提取title year jounal三个属性
            for keys, values in dic.items():
                if keys.strip() == "title":
                    data = data + toString(bib) + '%' + values.strip().strip(',').strip('{').strip('}')
            for keys, values in dic.items():
                if keys.strip() == "year":
                    data = data + '%' + values.strip().strip(',').strip('{').strip('}')
            for keys, values in dic.items():
                if flag == 1:
                    break
                elif keys.strip() == "journal":
                    data = data + '%' + values.strip().strip(',').strip('{').strip('}')
                    flag = 1
                elif keys.strip() == "booktitle":
                    data = data + '%' + values.strip().strip(',').strip('{').strip('}')           
                    flag = 1
                else:
                    flag = 0
            if flag == 0:
                data = data + '%' + 'null'
            print("data", data)
            data = data.split('%')
            path = ".\\result.xls"
            saveOneData(path, data, num)
        num = num + 1
    print(errors)
```


​

